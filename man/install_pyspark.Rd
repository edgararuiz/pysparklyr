% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/install.R
\name{install_pyspark}
\alias{install_pyspark}
\alias{install_databricks}
\title{Installs Pyspark and Python dependencies}
\usage{
install_pyspark(
  version = NULL,
  envname = paste("r-sparklyr-pyspark", version, sep = ifelse(is.null(version), "", "-")),
  python_version = ">=3.9",
  new_env = TRUE,
  method = c("auto", "virtualenv", "conda"),
  ...
)

install_databricks(
  version = NULL,
  envname = paste("r-sparklyr-databricks", version, sep = ifelse(is.null(version), "",
    "-")),
  python_version = ">=3.9",
  new_env = TRUE,
  method = c("auto", "virtualenv", "conda"),
  ...
)
}
\arguments{
\item{version}{Version of 'databricks.connect' to install}

\item{envname}{The name of the Python Environment to use to install the
Python libraries. Defaults to "r-sparklyr".}

\item{python_version}{The version of Python to use to create the Python
environment.}

\item{new_env}{If \code{TRUE}, any existing Python virtual environment and/or
Conda environment specified by \code{envname} is deleted first.}

\item{method}{The installation method to use. If creating a new environment,
\code{"auto"} (the default) is equivalent to \code{"virtualenv"}. Otherwise \code{"auto"}
infers the installation method based on the type of Python environment
specified by \code{envname}.}

\item{...}{Passed on to \code{\link[reticulate:py_install]{reticulate::py_install()}}}
}
\description{
Installs Pyspark and Python dependencies

Installs Databricks Connect and Python dependencies
}
