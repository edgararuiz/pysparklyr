% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/install.R
\name{install_pyspark}
\alias{install_pyspark}
\alias{install_databricks}
\title{Installs Pyspark and Python dependencies}
\usage{
install_pyspark(
  version = NULL,
  envname = NULL,
  python_version = ">=3.9",
  new_env = TRUE,
  method = c("auto", "virtualenv", "conda"),
  ...
)

install_databricks(
  version = NULL,
  cluster_id = NULL,
  envname = NULL,
  python_version = ">=3.9",
  new_env = TRUE,
  method = c("auto", "virtualenv", "conda"),
  ...
)
}
\arguments{
\item{version}{Version of 'databricks.connect' to install}

\item{envname}{The name of the Python Environment to use to install the
Python libraries. Default to \code{NULL.} If \code{NULL}, a name will automatically
be assigned based on the version that will be installed}

\item{python_version}{The version of Python to use to create the Python
environment.}

\item{new_env}{If \code{TRUE}, any existing Python virtual environment and/or
Conda environment specified by \code{envname} is deleted first.}

\item{method}{The installation method to use. If creating a new environment,
\code{"auto"} (the default) is equivalent to \code{"virtualenv"}. Otherwise \code{"auto"}
infers the installation method based on the type of Python environment
specified by \code{envname}.}

\item{...}{Passed on to \code{\link[reticulate:py_install]{reticulate::py_install()}}}

\item{cluster_id}{Target of the cluster ID that will be used with.
If provided, this value will be used to extract the cluster's
version}
}
\description{
Installs Pyspark and Python dependencies

Installs Databricks Connect and Python dependencies
}
