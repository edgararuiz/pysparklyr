---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# pysparklyr

<!-- badges: start -->

[![R-CMD-check](https://github.com/mlverse/pysparklyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/mlverse/pysparklyr/actions/workflows/R-CMD-check.yaml)
[![Spark-Connect](https://github.com/mlverse/pysparklyr/actions/workflows/spark-tests.yaml/badge.svg)](https://github.com/mlverse/pysparklyr/actions/workflows/spark-tests.yaml)
[![codecov](https://codecov.io/gh/mlverse/pysparklyr/graph/badge.svg?token=O1N9qPabpF)](https://codecov.io/gh/mlverse/pysparklyr)
<!-- badges: end -->

Integrates `sparklyr` with PySpark and Databricks. The main reason of this 
package is because the new Spark and Databricks Connect connection method does
not work with standard `sparklyr` integration.

To learn how to use, please visit the Spark / Databricks Connect article, 
available in the official `sparklyr` website: [Spark Connect, and Databricks Connect v2
](https://spark.rstudio.com/deployment/databricks-spark-connect.html)
